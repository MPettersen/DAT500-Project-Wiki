{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Spark implementation of Page Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "from operator import add\n",
    "import pyspark\n",
    "import os\n",
    "import glob\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_linked_path = '/mnt/wiktorskit-danielb-ns0000k/home/notebook/group04/clean_data/test_linked.csv'\n",
    "test_nolinked_path = '/mnt/wiktorskit-danielb-ns0000k/home/notebook/group04/clean_data/test_nolinked.csv'\n",
    "test_unlinked_path = '/mnt/wiktorskit-danielb-ns0000k/home/notebook/group04/clean_data/test_unlinked.csv'\n",
    "test_extralinks_path = '/mnt/wiktorskit-danielb-ns0000k/home/notebook/group04/clean_data/test_extralinks.csv'\n",
    "test_mostly_unlinked_path = '/mnt/wiktorskit-danielb-ns0000k/home/notebook/group04/clean_data/test_mostly_unlinked.csv'\n",
    "\n",
    "input_path = '/mnt/wiktorskit-danielb-ns0000k/home/notebook/group04/clean_data/wiki_test.csv'\n",
    "multi_input = '/mnt/wiktorskit-danielb-ns0000k/home/notebook/group04/clean_data/enwiki*.csv'\n",
    "nowiki_path = '/mnt/wiktorskit-danielb-ns0000k/home/notebook/group04/clean_data/nowiki*.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_test_linked_path = '/mnt/wiktorskit-danielb-ns0000k/home/notebook/group04/PR/validation_linked_01'\n",
    "output_test_unlinked_path = '/mnt/wiktorskit-danielb-ns0000k/home/notebook/group04/PR/validation_unlinked_01'\n",
    "output_test_extralinks_path = '/mnt/wiktorskit-danielb-ns0000k/home/notebook/group04/PR/validation_extralinks_01'\n",
    "output_test_mostly_unlinked_path = '/mnt/wiktorskit-danielb-ns0000k/home/notebook/group04/PR/validation_mostly_unlinked_01'\n",
    "\n",
    "output_path_enwiki_subset = '/mnt/wiktorskit-danielb-ns0000k/home/notebook/group04/PR/page_ranks_enwiki_subset_06'\n",
    "output_path_enwiki = '/mnt/wiktorskit-danielb-ns0000k/home/notebook/group04/PR/page_ranks_enwiki_01'\n",
    "output_path_nowiki = '/mnt/wiktorskit-danielb-ns0000k/home/notebook/group04/PR/page_ranks_nowiki_final'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPages(path):\n",
    "    \"\"\"\n",
    "    Reads a text file into a spark RDD if path is valid\n",
    "    \n",
    "    :param path: path of text file\n",
    "    :return: RDD containing the text file\n",
    "    \"\"\"\n",
    "    # Validity check of the path\n",
    "    for file in glob.glob(path):\n",
    "        if not os.path.isfile(file) or '/mnt/' not in file:\n",
    "            print('Not a valid path')\n",
    "            sys.exit(-1)\n",
    "    return sc.textFile('file://'+path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLinks(page):\n",
    "    \"\"\"\n",
    "    This function retrieves the links from any given page\n",
    "    \n",
    "    :param page: a page represented as a tab seperated string\n",
    "    :return: list of links belonging to page\n",
    "    \"\"\"\n",
    "    num = int(page.strip().split('\\t')[3])\n",
    "    if num == 0:\n",
    "        return []\n",
    "    else:\n",
    "        return page.strip().split('\\t')[-num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterFile(links):\n",
    "    \"\"\"\n",
    "    This function removes all links that link to files.\n",
    "    \n",
    "    :param links: list of links\n",
    "    :return: list of filtered links\n",
    "    \"\"\"\n",
    "    return [link for link in links if not (link.find('File:') == 0) and not (link.find('Wikipedia:') == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeContribs(links, rank):\n",
    "    \"\"\"\n",
    "    Calculates the link contributions to the rank of other pages links.\n",
    "    \n",
    "    Adapted from the computeContribs funtion from:\n",
    "        https://github.com/apache/spark/blob/master/examples/src/main/python/pagerank.py\n",
    "        \n",
    "    :param links: list of links belonging to a page\n",
    "    :param rank: the rank of the page the list of links belongs to\n",
    "    :return: tuples with links and their rank contribution\n",
    "    \"\"\"\n",
    "    num_links = len(links)\n",
    "    if num_links > 0:\n",
    "        for link in links:\n",
    "            yield (link, rank / num_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolated functions for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterLinks(all_pages, links):\n",
    "    filtered_links = []\n",
    "    for link in links:\n",
    "        if link in all_pages:\n",
    "            filtered_links.append(link)\n",
    "    return filtered_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing dead links and sink pages\n",
    "\n",
    "Dead links are links that link to pages outside the dataset.\n",
    "Sink pages are pages without outgoing links.\n",
    "\n",
    "Removing these links must be done on three steps.\n",
    "1. Remove dead links\n",
    "  1. Get list of all pages in dataset\n",
    "  2. Filter out all links that does not link to a page in the list\n",
    "2. Remove sink pages\n",
    "  1. Filter out pages with no outgoing links\n",
    "3. Remove links to sink pages\n",
    "  1. Get updated list of all pages in dataset\n",
    "  2. Filter out all links that linked to a sink page\n",
    "\n",
    "It is important to do it in this order. Because when removing dead links there is a chance of creating more sink pages. Therefore you must first remove the dead links, then the sink pages and finally the links to the sink pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPagePairs(pages):\n",
    "    \"\"\"\n",
    "    Functions that retrieves the a list of each page with their outgoing links\n",
    "    \n",
    "    :param pages: spark RDD containg the input file\n",
    "    :return: tuples of (page, [list of links])\n",
    "    \"\"\"\n",
    "    # The filter removes empyt lines that may occur in the csv file\n",
    "    # The map function maps the title of the page to the all the internal links in the page\n",
    "    page_pairs = (pages\n",
    "                  .filter(lambda line: len(line.strip()) > 0 and\n",
    "                          not (line.strip().split('\\t')[1].find('File:') == 0) and\n",
    "                          not (line.strip().split('\\t')[1].find('Wikipedia:') == 0))\n",
    "                  .map(lambda line: (line.strip().split('\\t')[1],\n",
    "                                     filterFile(getLinks(line)))))\n",
    "    # Removing links that link outside the dataset\n",
    "    # and then removes all sink pages (pages with no outgoing lins)\n",
    "    all_pages = page_pairs.map(lambda pair: pair[0]).collect()\n",
    "    page_pairs = (page_pairs\n",
    "                  .map(lambda pair: (pair[0], filterLinks(all_pages, pair[1])))\n",
    "                  .filter(lambda pair: len(pair[1]) > 0))\n",
    "    # Removing all links that linked to sink pages\n",
    "    all_pages = page_pairs.map(lambda pair: pair[0]).collect()\n",
    "    return page_pairs.map(lambda pair: (pair[0], filterLinks(all_pages, pair[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getN(page_pairs):\n",
    "    \"\"\"\n",
    "    Function that returns the number of page pairs\n",
    "    \n",
    "    :param page_pairs: tuples of page pairs\n",
    "    :return: number of pages\n",
    "    \"\"\"\n",
    "    return page_pairs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeRanks(page_pairs, N):\n",
    "    \"\"\"\n",
    "    Initializes the ranks for the pages in the dataset\n",
    "    \n",
    "    :param page_paris: tuples of page pairs\n",
    "    :param N: number of pages\n",
    "    :return: tuples of (page, initial rank)\n",
    "    \"\"\"\n",
    "    return page_pairs.map(lambda pair: (pair[0], 1.0/N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculatePageRanks(page_pairs, ranks, N, npart=12, eps=1e-4):\n",
    "    \"\"\"\n",
    "    A function that iteratively calculates the page ranks until convergence\n",
    "    \n",
    "    :param page_pairs: tuples of page pairs\n",
    "    :param ranks: tuples of (page, initial rank)\n",
    "    :param N: number of pages\n",
    "    :return: tuples of page ranks (page, final rank)\n",
    "    \"\"\"\n",
    "    num_iter = 0\n",
    "    while True:\n",
    "        num_iter += 1\n",
    "        contribs = procRDD(page_pairs.join(ranks, npart).flatMap(\n",
    "            lambda page_links_rank:\n",
    "                computeContribs(page_links_rank[1][0], page_links_rank[1][1])))\n",
    "        ranks_previous = procRDD(ranks)\n",
    "        ranks = procRDD(ranks\n",
    "                        .leftOuterJoin(contribs.reduceByKey(add), npart)\n",
    "                        .mapValues(lambda rank: \n",
    "                                   rank[1] * .85 + .15/N if rank[1]\n",
    "                                   else (rank[0]/N) * .85 + .15/N))\n",
    "        convergence = (ranks_previous\n",
    "                       .join(ranks, npart)\n",
    "                       .map(lambda rank: abs(rank[1][0]-rank[1][1]))\n",
    "                       .sum())\n",
    "        if convergence < eps:\n",
    "            ranks = procRDD(ranks_previous\n",
    "                            .leftOuterJoin(contribs.reduceByKey(add), npart)\n",
    "                            .mapValues(lambda rank:\n",
    "                                   rank[1] * .85 + .15/N if rank[1]\n",
    "                                   else 0))\n",
    "            break\n",
    "        \n",
    "    return page_pairs.join(ranks).map(\n",
    "        lambda page_rank: (page_rank[0], page_rank[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRanksSum(page_ranks):\n",
    "    \"\"\"\n",
    "    Function that adds the rank of all pages. In a perfect system the sum should be 1.\n",
    "    But if there are outgoing links to pages not in the dataset or if there are pages\n",
    "    without incoming links the sum will not be 1, but should still be somewhat close.\n",
    "    \n",
    "    :param page_ranks: tuples of (page, rank)\n",
    "    :return: sum of all ranks\n",
    "    \"\"\"\n",
    "    return page_ranks.map(lambda rank: rank[1]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to handle partioning for iterative joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procRDD(rdd, cache=True, part=True, hashp=True, npart=12):\n",
    "    \"\"\"\n",
    "    Helper to handle caching/partioning\n",
    "    \n",
    "    Function taken from:\n",
    "    https://stackoverflow.com/questions/31659404/spark-iteration-time-increasing-exponentially-when-using-join\n",
    "    \n",
    "    :param rdd: pyspark RDD\n",
    "    :param cache: boolean (default=True)\n",
    "    :param part: boolean (default=True)\n",
    "    :param hashp: boolean (default=True)\n",
    "    :param npart: number of partitions (default=12 suggested to be 2*(number of cores))\n",
    "    :return: rdd or rdd.cache()\n",
    "    \"\"\"\n",
    "    rdd = rdd if not part else rdd.repartition(npart)\n",
    "    rdd = rdd if not hashp else rdd.partitionBy(npart)\n",
    "    return rdd if not cache else rdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark.SparkContext.setSystemProperty('spark.executor.memory', '7g')\n",
    "pyspark.SparkContext.setSystemProperty('spark.driver.cores', '2')\n",
    "pyspark.SparkContext.setSystemProperty('spark.driver.memory', '7g')\n",
    "pyspark.SparkContext.setSystemProperty('spark.cleaner.ttl', '180')\n",
    "sc = pyspark.SparkContext(appName='pagerank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "enwiki_pages = getPages(multi_input)\n",
    "nowiki_pages = getPages(nowiki_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_pairs_enwiki = getPagePairs(enwiki_pages)\n",
    "page_pairs_nowiki = getPagePairs(nowiki_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "enwiki = enwiki_pages.map(lambda line: line.strip().split('\\t')[0])\n",
    "nowiki = nowiki_pages.map(lambda line: line.strip().split('\\t')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_enwiki = getN(enwiki)\n",
    "N_nowiki = getN(nowiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11132071, 698181)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_enwiki, N_nowiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test of the validity of the algorithm under various circumstances\n",
    "\n",
    "page_pairs_linked:\n",
    "\n",
    "All pages have incoming links and all outgoing links links to pages in set\n",
    "\n",
    "PAGE | LINKS\n",
    "-----|-------\n",
    "A    | B C D\n",
    "B    | A C\n",
    "C    | A E\n",
    "D    | A B C\n",
    "E    | A D\n",
    "\n",
    "page_pairs_unlinked:\n",
    "\n",
    "All outgoing links links to pages in set, but not all pages in set have incoming links\n",
    "\n",
    "PAGE | LINKS\n",
    "-----|-------\n",
    "A    | B C D\n",
    "B    | A C\n",
    "C    | A\n",
    "D    | A B C\n",
    "E    | A D\n",
    "\n",
    "page_pairs_extralinks:\n",
    "\n",
    "All pages have incoming links, but some links links to pages outside of the set\n",
    "\n",
    "PAGE | LINKS\n",
    "-----|--------\n",
    "A    | B C D\n",
    "B    | A C H J\n",
    "C    | A E K L\n",
    "D    | A B C\n",
    "E    | A D\n",
    "\n",
    "page_pairs_mostly_unlinked:\n",
    "\n",
    "All outgoing links links to pages in set, but most pages in set does not have incoming links\n",
    "\n",
    "PAGE | LINKS\n",
    "-----|-------\n",
    "A    | B C D\n",
    "B    | A C\n",
    "C    | A\n",
    "D    | A B C\n",
    "E    | A D\n",
    "F    | B D\n",
    "G    | A\n",
    "H    | A B C\n",
    "I    | C D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark.SparkContext.setSystemProperty('spark.executor.memory', '7g')\n",
    "pyspark.SparkContext.setSystemProperty('spark.driver.cores', '2')\n",
    "pyspark.SparkContext.setSystemProperty('spark.driver.memory', '7g')\n",
    "pyspark.SparkContext.setSystemProperty('spark.cleaner.ttl', '180')\n",
    "sc = pyspark.SparkContext(appName='pagerank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_linked_pages = getPages(test_linked_path)\n",
    "test_nolinked_pages = getPages(test_nolinked_path)\n",
    "test_unlinked_pages = getPages(test_unlinked_path)\n",
    "test_extralinks_pages = getPages(test_extralinks_path)\n",
    "test_mostly_unlinked_pages = getPages(test_mostly_unlinked_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_pairs_linked = getPagePairs(test_linked_pages)\n",
    "page_pairs_nolinked = getPagePairs(test_nolinked_pages)\n",
    "page_pairs_unlinked = getPagePairs(test_unlinked_pages)\n",
    "page_pairs_extralinks = getPagePairs(test_extralinks_pages)\n",
    "page_pairs_mostly_unlinked = getPagePairs(test_mostly_unlinked_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['A', 'B', 'C', 'D', 'E'], list)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_pairs_linked.map(lambda pair: pair[0]).collect(), type(page_pairs_linked.map(lambda pair: pair[0]).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('linked',\n",
       " [('A', ['B', 'C', 'D']),\n",
       "  ('B', ['A', 'C']),\n",
       "  ('C', ['A', 'E']),\n",
       "  ('D', ['A', 'B', 'C']),\n",
       "  ('E', ['A', 'D'])],\n",
       " 'nolinked',\n",
       " [('A', ['B', 'C', 'D']),\n",
       "  ('B', ['A', 'C']),\n",
       "  ('C', ['A']),\n",
       "  ('D', ['A', 'B', 'C'])],\n",
       " 'unlinked',\n",
       " [('A', ['B', 'C', 'D']),\n",
       "  ('B', ['A', 'C']),\n",
       "  ('C', ['A']),\n",
       "  ('D', ['A', 'B', 'C']),\n",
       "  ('E', ['A', 'D'])],\n",
       " 'extra',\n",
       " [('A', ['B', 'C', 'D']),\n",
       "  ('B', ['A', 'C']),\n",
       "  ('C', ['A', 'E']),\n",
       "  ('D', ['A', 'B', 'C']),\n",
       "  ('E', ['A', 'D'])],\n",
       " 'many unlinked',\n",
       " [('A', ['B', 'C', 'D']),\n",
       "  ('B', ['A', 'C']),\n",
       "  ('C', ['A']),\n",
       "  ('D', ['A', 'B', 'C']),\n",
       "  ('E', ['A', 'D']),\n",
       "  ('F', ['B', 'D']),\n",
       "  ('G', ['A']),\n",
       "  ('H', ['A', 'B', 'C']),\n",
       "  ('I', ['C', 'D'])])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'linked', page_pairs_linked.collect(), 'nolinked', page_pairs_nolinked.collect(), 'unlinked', page_pairs_unlinked.collect(), 'extra', page_pairs_extralinks.collect(), 'many unlinked', page_pairs_mostly_unlinked.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_linked = getN(page_pairs_linked)\n",
    "N_nolinked = getN(page_pairs_nolinked)\n",
    "N_unlinked = getN(page_pairs_unlinked)\n",
    "N_extra = getN(page_pairs_extralinks)\n",
    "N_mostly_unlinked = getN(page_pairs_mostly_unlinked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 4, 5, 5, 9)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_linked, N_nolinked, N_unlinked, N_extra, N_mostly_unlinked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks_linked = initializeRanks(page_pairs_linked, N_linked)\n",
    "ranks_nolinked = initializeRanks(page_pairs_nolinked, N_nolinked)\n",
    "ranks_unlinked = initializeRanks(page_pairs_unlinked, N_unlinked)\n",
    "ranks_extra = initializeRanks(page_pairs_extralinks, N_extra)\n",
    "ranks_mostly_unlinked = initializeRanks(page_pairs_mostly_unlinked, N_mostly_unlinked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_ranks_linked = calculatePageRanks(page_pairs_linked, ranks_linked, N_linked)\n",
    "page_ranks_nolinked = calculatePageRanks(page_pairs_nolinked, ranks_nolinked, N_nolinked)\n",
    "page_ranks_unlinked = calculatePageRanks(page_pairs_unlinked, ranks_unlinked, N_unlinked)\n",
    "page_ranks_extra = calculatePageRanks(page_pairs_extralinks, ranks_extra, N_extra)\n",
    "page_ranks_mostly_unlinked = calculatePageRanks(page_pairs_mostly_unlinked, ranks_mostly_unlinked, N_mostly_unlinked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('linked',\n",
       " [('D', 0.17062566477346572),\n",
       "  ('A', 0.30233451660549926),\n",
       "  ('C', 0.23370635742716941),\n",
       "  ('E', 0.12933060638071578),\n",
       "  ('B', 0.16400285481314963)],\n",
       " 'nolinked',\n",
       " [('D', 0.1481819841284264),\n",
       "  ('A', 0.39066040762631127),\n",
       "  ('C', 0.27099187470561176),\n",
       "  ('B', 0.19016573353965016)],\n",
       " 'unlinked',\n",
       " [('D', 0.15731831479952635),\n",
       "  ('A', 0.3951524436171989),\n",
       "  ('C', 0.26581327875089417),\n",
       "  ('B', 0.18653523581166995),\n",
       "  ('E', 0)],\n",
       " 'extra',\n",
       " [('D', 0.17062566477346572),\n",
       "  ('A', 0.30233451660549926),\n",
       "  ('C', 0.23370635742716941),\n",
       "  ('B', 0.16400285481314963),\n",
       "  ('E', 0.12933060638071578)],\n",
       " 'many unlinked',\n",
       " [('I', 0),\n",
       "  ('D', 0.14824504749182213),\n",
       "  ('A', 0.3815948584312732),\n",
       "  ('G', 0),\n",
       "  ('C', 0.2562529047416004),\n",
       "  ('F', 0),\n",
       "  ('B', 0.17982402638834158),\n",
       "  ('E', 0),\n",
       "  ('H', 0)])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'linked', page_ranks_linked.collect(), 'nolinked', page_ranks_nolinked.collect(), 'unlinked', page_ranks_unlinked.collect(), 'extra', page_ranks_extra.collect(), 'many unlinked', page_ranks_mostly_unlinked.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks_sum_linked = getRanksSum(page_ranks_linked)\n",
    "ranks_sum_nolinked = getRanksSum(page_ranks_nolinked)\n",
    "ranks_sum_unlinked = getRanksSum(page_ranks_unlinked)\n",
    "ranks_sum_extra = getRanksSum(page_ranks_extra)\n",
    "ranks_sum_mostly_unlinked = getRanksSum(page_ranks_mostly_unlinked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9999999999999998,\n",
       " 0.9999999999999997,\n",
       " 1.0048192729792893,\n",
       " 0.9999999999999998,\n",
       " 0.9659168370530374)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks_sum_linked, ranks_sum_nolinked, ranks_sum_unlinked, ranks_sum_extra, ranks_sum_mostly_unlinked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "page_pairs_linked:\n",
    "\n",
    "PAGE | LINKS | PAGE RANK\n",
    "-----|-------|--------------------\n",
    "A    | B C D | 0.30233451660549926\n",
    "B    | A C   | 0.16400285481314963\n",
    "C    | A E   | 0.23370635742716941\n",
    "D    | A B C | 0.17062566477346572\n",
    "E    | A D   | 0.12933060638071578\n",
    "TOTAL| RANK  | 0.9999999999999999\n",
    "\n",
    "page_pairs_unlinked:\n",
    "\n",
    "PAGE | LINKS | PAGE RANK\n",
    "-----|-------|--------------------\n",
    "A    | B C D | 0.39515244361719903\n",
    "B    | A C   | 0.18653523581166997\n",
    "C    | A     | 0.26581327875089417\n",
    "D    | A B C | 0.15731831479952638\n",
    "E    | A D   | 0.03614458161656832\n",
    "TOTAL| RANK  | 1.040963854595858\n",
    "\n",
    "page_pairs_extralinks:\n",
    "\n",
    "PAGE | LINKS   | PAGE RANK\n",
    "-----|---------|--------------------\n",
    "A    | B C D   | 0.12030966188618383\n",
    "B    | A C H J | 0.0894076291857128\n",
    "C    | A E K L | 0.10859879786388368\n",
    "D    | A B C   | 0.08728132127189267\n",
    "E    | A D     | 0.05333157300527242\n",
    "TOTAL| RANK    | 0.45892898321294545\n",
    "\n",
    "page_pairs_mostly_unlinked:\n",
    "\n",
    "PAGE | LINKS | PAGE RANK\n",
    "-----|-------|---------------------\n",
    "A    | B C D | 0.38159485843127317\n",
    "B    | A C   | 0.17982402638834158\n",
    "C    | A     | 0.2562529047416004\n",
    "D    | A B C | 0.14824504749182213\n",
    "E    | A D   | 0.018404907980694597\n",
    "F    | B D   | 0.018404907980694597\n",
    "G    | A     | 0.018404907980694597\n",
    "H    | A B C | 0.018404907980694597\n",
    "I    | C D   | 0.018404907980694597\n",
    "TOTAL| RANK  | 1.0579413769565098"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PageRank of norwegian wiki dump\n",
    "\n",
    "## Initial attempt\n",
    "Attempt without manual control of partioning of join operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence at iteration 1: 1.0168933004\n",
      "Convergence at iteration 2: 0.3969783753\n",
      "Convergence at iteration 3: 0.2025033943\n",
      "Convergence at iteration 4: 0.1226482849\n",
      "Convergence at iteration 5: 0.0762758779\n",
      "Convergence at iteration 6: 0.0474383168\n",
      "0.5282880031918347\n",
      "Total execution time was: 488 seconds\n"
     ]
    }
   ],
   "source": [
    "pyspark.SparkContext.setSystemProperty('spark.executor.memory', '7g')\n",
    "pyspark.SparkContext.setSystemProperty('spark.driver.cores', '2')\n",
    "pyspark.SparkContext.setSystemProperty('spark.driver.memory', '7g')\n",
    "pyspark.SparkContext.setSystemProperty('spark.cleaner.ttl', '180')\n",
    "sc = pyspark.SparkContext(appName='pagerank')\n",
    "start = time()\n",
    "eps = 1e-2\n",
    "npart = 48\n",
    "pages = getPages(nowiki_path)\n",
    "page_pairs = pages \\\n",
    "    .filter(lambda line: len(line.strip()) > 0 and \\\n",
    "           not (line.strip().split('\\t')[1].find('File:') == 0) and \\\n",
    "           not (line.strip().split('\\t')[1].find('Wikipedia:') == 0)) \\\n",
    "    .map(lambda line: (line.strip().split('\\t')[1], \\\n",
    "                       filterFile(getLinks(line))))\n",
    "N = page_pairs.count()\n",
    "ranks = page_pairs.map(lambda pair: (pair[0], 1.0/N))\n",
    "num_iter = 0\n",
    "\n",
    "while True:\n",
    "    num_iter += 1\n",
    "    contribs = page_pairs.join(ranks).flatMap(\n",
    "        lambda page_links_rank:\n",
    "            computeContribs(page_links_rank[1][0], page_links_rank[1][1]))\n",
    "    ranks_previous = procRDD(ranks)\n",
    "    ranks = procRDD(ranks \\\n",
    "        .leftOuterJoin(contribs.reduceByKey(add)) \\\n",
    "        .mapValues(lambda rank: \n",
    "                   rank[1] * .85 + .15/N if rank[1] \n",
    "                   else (rank[0]/N) * .85 + .15/N))\n",
    "    convergence = ranks_previous \\\n",
    "        .join(ranks) \\\n",
    "        .map(lambda rank: abs(rank[1][0]-rank[1][1])) \\\n",
    "        .sum()\n",
    "    print('Convergence at iteration %s: %s' % (num_iter, round(convergence, 10)))\n",
    "    if convergence < eps or num_iter > 5:\n",
    "        break\n",
    "\n",
    "page_ranks = page_pairs.join(ranks).map(\n",
    "    lambda page_rank: (page_rank[0], page_rank[1][1]))\n",
    "\n",
    "ranks_sum = page_ranks.map(lambda rank: rank[1]).sum()\n",
    "print(ranks_sum)\n",
    "end = time()\n",
    "print('Total execution time was:', round(end-start), 'seconds')\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With partioning\n",
    "Attempt with manual partitioning of join operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence at iteration 1: 1.0168933004\n",
      "Convergence at iteration 2: 0.3969783753\n",
      "Convergence at iteration 3: 0.2025033943\n",
      "Convergence at iteration 4: 0.1226482849\n",
      "Convergence at iteration 5: 0.0762758779\n",
      "Convergence at iteration 6: 0.0474383168\n",
      "Convergence at iteration 7: 0.0314807382\n",
      "Convergence at iteration 8: 0.0209964926\n",
      "Convergence at iteration 9: 0.0136085296\n",
      "Convergence at iteration 10: 0.0090386682\n",
      "0.4935836470026831\n",
      "Total execution time was: 1557 seconds\n"
     ]
    }
   ],
   "source": [
    "pyspark.SparkContext.setSystemProperty('spark.executor.memory', '7g')\n",
    "pyspark.SparkContext.setSystemProperty('spark.driver.cores', '2')\n",
    "pyspark.SparkContext.setSystemProperty('spark.driver.memory', '7g')\n",
    "pyspark.SparkContext.setSystemProperty('spark.cleaner.ttl', '180')\n",
    "sc = pyspark.SparkContext(appName='pagerank')\n",
    "start = time()\n",
    "eps = 1e-2\n",
    "npart = 48\n",
    "# The filter removes empyt lines that may occur in the csv file\n",
    "# The map function maps the title of the page to the all the internal links in the page\n",
    "pages = getPages(nowiki_path)\n",
    "page_pairs = pages \\\n",
    "    .filter(lambda line: len(line.strip()) > 0 and \\\n",
    "           not (line.strip().split('\\t')[1].find('File:') == 0) and \\\n",
    "           not (line.strip().split('\\t')[1].find('Wikipedia:') == 0)) \\\n",
    "    .map(lambda line: (line.strip().split('\\t')[1], \\\n",
    "                       filterFile(getLinks(line))))\n",
    "N = page_pairs.count()\n",
    "ranks = page_pairs.map(lambda pair: (pair[0], 1.0/N))\n",
    "num_iter = 0\n",
    "\n",
    "while True:\n",
    "    num_iter += 1\n",
    "    contribs = page_pairs.join(ranks, npart).flatMap(\n",
    "        lambda page_links_rank:\n",
    "            computeContribs(page_links_rank[1][0], page_links_rank[1][1]))\n",
    "    ranks_previous = ranks\n",
    "    ranks = ranks \\\n",
    "        .leftOuterJoin(contribs.reduceByKey(add), npart) \\\n",
    "        .mapValues(lambda rank: \n",
    "                   rank[1] * .85 + .15/N if rank[1] \n",
    "                   else (rank[0]/N) * .85 + .15/N)\n",
    "    convergence = ranks_previous \\\n",
    "        .join(ranks, npart) \\\n",
    "        .map(lambda rank: abs(rank[1][0]-rank[1][1])) \\\n",
    "        .sum()\n",
    "    print('Convergence at iteration %s: %s' % (num_iter, round(convergence, 10)))\n",
    "    if convergence < eps:\n",
    "        break\n",
    "\n",
    "page_ranks = page_pairs.join(ranks, npart).map(\n",
    "    lambda page_rank: (page_rank[0], page_rank[1][1]))\n",
    "\n",
    "ranks_sum = page_ranks.map(lambda rank: rank[1]).sum()\n",
    "print(ranks_sum)\n",
    "end = time()\n",
    "print('Total execution time was:', round(end-start), 'seconds')\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final implementation with partitioning and caching\n",
    "\n",
    "Best performence w.r.t execution time, but caching may cause \"no space left on device\" errors when working with limited resources.\n",
    "\n",
    "The system property cleaner.ttl defines spark objects time to live. This is a vital component when working with limited resources. It needs to set to a value greater than the execution time of one iteration of the while loop, to have the desired effect. Preferably 1.5 times the execution time of one iteration.\n",
    "    \n",
    "Documentation for spark.cleaner.ttl:\n",
    ">Duration (seconds) of how long Spark will remember any metadata (stages generated, tasks generated, etc.). Periodic cleanups will ensure that metadata older than this duration will be forgetten. This is useful for running Spark for many hours / days (for example, running 24/7 in case of Spark Streaming applications).  Note that any RDD that persists in memory for more than this duration will be cleared as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence at iteration 1: 1.0168933004\n",
      "Convergence at iteration 2: 0.3969783753\n",
      "Convergence at iteration 3: 0.2025033943\n",
      "Convergence at iteration 4: 0.1226482849\n",
      "Convergence at iteration 5: 0.0762758779\n",
      "Convergence at iteration 6: 0.0474383168\n",
      "Convergence at iteration 7: 0.0314807382\n",
      "Convergence at iteration 8: 0.0209964926\n",
      "Convergence at iteration 9: 0.0136085296\n",
      "Convergence at iteration 10: 0.0090386682\n",
      "0.49358364700269325\n",
      "Total execution time was: 613 seconds\n",
      "Total number of pages in dataset: 679871\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    pyspark.SparkContext.setSystemProperty('spark.executor.memory', '7g')\n",
    "    pyspark.SparkContext.setSystemProperty('spark.driver.cores', '2')\n",
    "    pyspark.SparkContext.setSystemProperty('spark.driver.memory', '7g')\n",
    "    pyspark.SparkContext.setSystemProperty('spark.cleaner.ttl', '180')\n",
    "    sc = pyspark.SparkContext(appName='pagerank')\n",
    "    start = time()\n",
    "    eps = 1e-2\n",
    "    npart = 48\n",
    "    # The filter removes empyt lines that may occur in the csv file\n",
    "    # The map function maps the title of the page to the all the internal links in the page\n",
    "    pages = getPages(nowiki_path)\n",
    "    page_pairs = procRDD(pages \\\n",
    "        .filter(lambda line: len(line.strip()) > 0 and \\\n",
    "               not (line.strip().split('\\t')[1].find('File:') == 0) and \\\n",
    "               not (line.strip().split('\\t')[1].find('Wikipedia:') == 0)) \\\n",
    "        .map(lambda line: (line.strip().split('\\t')[1], \\\n",
    "                           filterFile(getLinks(line)))), npart=npart)\n",
    "    N = page_pairs.count()\n",
    "    ranks = page_pairs.map(lambda pair: (pair[0], 1.0/N))\n",
    "    num_iter = 0\n",
    "    \n",
    "    while True:\n",
    "        num_iter += 1\n",
    "        contribs = page_pairs.join(ranks, npart).flatMap(\n",
    "            lambda page_links_rank:\n",
    "                computeContribs(page_links_rank[1][0], page_links_rank[1][1]))\n",
    "        ranks_previous = procRDD(ranks, npart=npart)\n",
    "        ranks = procRDD(ranks \\\n",
    "            .leftOuterJoin(contribs.reduceByKey(add), npart) \\\n",
    "            .mapValues(lambda rank: \n",
    "                       rank[1] * .85 + .15/N if rank[1] \n",
    "                       else (rank[0]/N) * .85 + .15/N), npart=npart)\n",
    "        convergence = ranks_previous \\\n",
    "            .join(ranks, npart) \\\n",
    "            .map(lambda rank: abs(rank[1][0]-rank[1][1])) \\\n",
    "            .sum()\n",
    "        print('Convergence at iteration %s: %s' % (num_iter, round(convergence, 10)))\n",
    "        if convergence < eps:\n",
    "            ranks = procRDD(ranks_previous\n",
    "                            .leftOuterJoin(contribs.reduceByKey(add), npart)\n",
    "                            .mapValues(lambda rank:\n",
    "                                   rank[1] * .85 + .15/N if rank[1]\n",
    "                                   else 0))\n",
    "            break\n",
    "        else:\n",
    "            page_pairs = page_pairs.cache()\n",
    "    \n",
    "    \n",
    "    page_ranks = page_pairs.join(ranks, npart).map(\n",
    "        lambda page_rank: (page_rank[0], page_rank[1][1]))\n",
    "    \n",
    "    ranks_sum = page_ranks.map(lambda rank: rank[1]).sum()\n",
    "    print(ranks_sum)\n",
    "    end = time()\n",
    "    print('Total execution time was:', round(end-start), 'seconds')\n",
    "    print('Total number of pages in dataset:', N)\n",
    "    \n",
    "    # Uncomment if results to be saved to file\n",
    "#     page_ranks.saveAsTextFile(output_path_nowiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Highest Ranked Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPageRank(line):\n",
    "    pair = line.split(',')\n",
    "    if len(pair) > 2:\n",
    "        return ','.join(pair[:-1])[2:], float(pair[-1].strip()[:-1])\n",
    "    else:\n",
    "        return pair[0][2:-1], float(pair[1].strip()[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark.SparkContext.setSystemProperty('spark.executor.memory', '7g')\n",
    "pyspark.SparkContext.setSystemProperty('spark.driver.cores', '2')\n",
    "pyspark.SparkContext.setSystemProperty('spark.driver.memory', '7g')\n",
    "pyspark.SparkContext.setSystemProperty('spark.cleaner.ttl', '600')\n",
    "sc = pyspark.SparkContext(appName='pagerank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_ranks = sc.textFile('file:///mnt/wiktorskit-danielb-ns0000k/home/notebook/group04/PR/page_ranks_nowiki_final/part*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_ranks = pages_ranks.map(lambda line: getPageRank(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Knole House', 2.714370680994686e-07),\n",
       " ('Kapitel', 2.2063036596032346e-07),\n",
       " ('Vorspiel', 2.637562177457272e-07),\n",
       " ('Nyhetsbyrå', 3.6482482045256115e-07),\n",
       " ('Rostrevor', 2.7067263659980717e-07)]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_ranks.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(679871,\n",
       " [('Kategori:Wikipedia-administrativa', 0.0031242702323758718),\n",
       "  ('Kategori:Kategorier', 0.0024946961753485814),\n",
       "  ('Kategori:Samfunn', 0.002282169055405368),\n",
       "  ('Kategori:Kategoriinnhold', 0.0017586968871229995),\n",
       "  ('Kategori:Geografi', 0.0016417855977165956),\n",
       "  ('Kategori:Struktur', 0.0016032405898556326),\n",
       "  ('Kategori:Tid', 0.0014818645713440173),\n",
       "  ('Kategori:Århundrer', 0.0013188511020142159),\n",
       "  ('Kategori:Kategorier etter område', 0.00131415971468974),\n",
       "  ('Kategori:Sport', 0.0011078784725969942)])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_ranking = page_ranks.sortBy(lambda a: a[1], False)\n",
    "sorted_ranking.count(), sorted_ranking.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark.SparkContext.setSystemProperty('spark.executor.memory', '7g')\n",
    "pyspark.SparkContext.setSystemProperty('spark.driver.cores', '2')\n",
    "pyspark.SparkContext.setSystemProperty('spark.driver.memory', '7g')\n",
    "pyspark.SparkContext.setSystemProperty('spark.cleaner.ttl', '600')\n",
    "sc = pyspark.SparkContext(appName='pagerank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_ranks = sc.textFile('file:///mnt/wiktorskit-danielb-ns0000k/home/notebook/group04/PR/page_ranks_enwiki_01/part*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_ranks = pages_ranks.map(lambda line: getPageRank(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Jamie Campbell (Scottish footballer)', 1.917980528360071e-08),\n",
       " ('Marouane Mrabet', 1.6227994845750827e-08),\n",
       " ('Carry cot', 1.6227994845750827e-08),\n",
       " ('Draft:Barry Twomlow', 1.6227994845750827e-08),\n",
       " ('Category:File-Class The Amazing Race articles', 1.6227994845750827e-08)]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_ranks.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9243471,\n",
       " [('Category:Categories by parameter', 0.0013003267398329944),\n",
       "  ('Category:Wikipedia categories', 0.0012249346062321908),\n",
       "  ('Category:Categories by location', 0.000962433864150088),\n",
       "  ('Category:Living people', 0.0008139243993796682),\n",
       "  ('Category:Main topic classifications', 0.0007176716849691154),\n",
       "  ('United States', 0.0006170628175589761),\n",
       "  ('Category:Continents', 0.0005736776471412021),\n",
       "  ('Category:Wikipedia administration', 0.0005318544402534897),\n",
       "  ('Category:Categories by country', 0.0005265113883494863),\n",
       "  ('Category:Categories by geographical categorization',\n",
       "   0.0005091010298417353)])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_ranking = page_ranks.sortBy(lambda a: a[1], False)\n",
    "sorted_ranking.count(), sorted_ranking.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
