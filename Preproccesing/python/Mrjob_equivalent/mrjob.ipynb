{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Briggstone\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mwparserfromhell as mwp\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_tokens_wo_stopwords (s):\n",
    "    punctuations = set(['.', ',', ';', ':', '?', '!', '#', '\\\\', '/', '\"', '\\'', '\\'\\'', '´´', '´', '``', '`', '(', ')'])\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filter = punctuations & stop_words\n",
    "    s = re.sub(r'([^\\s\\w]|_)+', ' ', s)\n",
    "    tokens = s.split()\n",
    "    remove = set(tokens) &  stop_words\n",
    "    tokens = [x for x in tokens if x.lower() not in remove]\n",
    "    return tokens\n",
    "\n",
    "def return_links (tl):\n",
    "    link_list = []\n",
    "    for l in tl:\n",
    "        link_list.append(str(l.title))\n",
    "    return link_list\n",
    "\n",
    "def handle_text_string (s):\n",
    "    s = mwp.parse(s)\n",
    "    links = s.filter_wikilinks()\n",
    "    links = return_links(links)\n",
    "    s = s.strip_code()\n",
    "    s = return_tokens_wo_stopwords(s)\n",
    "    return s,links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "articles_id = []\n",
    "articles_info = []\n",
    "\n",
    "with open('test14.xml-p7697599p7744799', encoding = \"utf_8\") as fp:\n",
    "    in_article = False\n",
    "    article_id = ''\n",
    "    title = \"\"\n",
    "    in_text = False\n",
    "    text = []\n",
    "    links = []\n",
    "    for line in fp:\n",
    "        line = line.strip()\n",
    "        if in_article:\n",
    "            if in_text:\n",
    "                if line.find('/text') != -1:\n",
    "                    in_text = False\n",
    "                    s = line[0:line.find('<')]\n",
    "                    s,l = handle_text_string(s)\n",
    "                else:\n",
    "                    s,l = handle_text_string(line)\n",
    "                text.extend(s)\n",
    "                links.extend(l)\n",
    "            elif line.find('</page>') != -1 and in_article:\n",
    "                in_article = False\n",
    "                articles_id.append(article_id)\n",
    "                article_info = []\n",
    "                article_info.append(title)\n",
    "                article_info.append(text)\n",
    "                article_info.append(links)\n",
    "                articles_info.append(article_info)\n",
    "                title = \"\"\n",
    "                text = []\n",
    "                links = []\n",
    "            elif line.find('<text') != -1:\n",
    "                if line.find('#REDIRECT') != -1:\n",
    "                        in_article = False\n",
    "                        title = \"\"\n",
    "                        id = \"\"\n",
    "                else:\n",
    "                    in_text = True               \n",
    "            elif line.find('<title>') != -1 and not title:\n",
    "                if line.find('Wikipedia:') != -1:\n",
    "                    in_article = False\n",
    "                else:\n",
    "                    i = line.find('>') + 1\n",
    "                    title = line[i: line.find('<', i)]\n",
    "            elif line.find('<id>') != -1 and not article_id:\n",
    "                i = line.find('>') + 1\n",
    "                article_id = line[i: line.find('<', i)]\n",
    "        elif line.find('<page>') != -1:\n",
    "            in_article = True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7697605\n",
      "['Konica Minolta Cup', ['Japan', 'LPGA', 'Championship', 'Konica', 'Minolta', 'Cup', 'golf', 'competition', 'WRU', 'Challenge', 'Cup', 'Welsh', 'rugby', 'union', 'competition', 'Konica', 'Cup', 'Minolta', 'merger', 'may', 'refer', 'Konica', 'Cup', 'football', 'football', 'competition'], ['Japan LPGA Championship', 'WRU Challenge Cup', 'Konica Cup (football)']]\n"
     ]
    }
   ],
   "source": [
    "print(articles_id[0])\n",
    "print(articles_info[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = json.dumps([{'article_id': article_id, 'article_info': article_info} for article_id, article_info in zip(articles_id, articles_info)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"article_id\": \"7697605\", \"article_info\": [\"Konica Minolta Cup\", [\"Japan\", \"LPGA\", \"Championship\", \"Konica\", \"Minolta\", \"Cup\", \"golf\", \"competition\", \"WRU\", \"Challenge\", \"Cup\", \"Welsh\", \"rugby\", \"union\", \"competition\", \"Konica\", \"Cup\", \"Minolta\", \"merger\", \"may\", \"refer\", \"Konica\", \"Cup\", \"football\", \"football\", \"competition\"], [\"Japan LPGA Championship\", \"WRU Challenge Cup\", \"Konica Cup (football)\"]]}, {\"article_id\": \"7697605\", \"article_info\": [\"Archer (typeface)\", [\"Infobox\", \"typeface\", \"name\", \"Archer\", \"image\", \"ArcherSpec\", \"svg\", \"style\", \"Serif\", \"classifications\", \"Humanist\", \"slab\", \"serif\", \"date\", \"2001\", \"creator\", \"Tobias\", \"Frere\", \"Jones\", \"br\", \"Jonathan\", \"Hoefler\", \"foundry\", \"Hoefler\", \"Frere\", \"Jones\", \"Archer\", \"slab\", \"serif\", \"typeface\", \"designed\", \"2001\", \"Tobias\", \"Frere\", \"Jones\", \"Jonathan\", \"Hoefler\", \"use\", \"Martha\", \"Stewart\", \"Living\", \"magazine\", \"It\", \"later\", \"released\", \"Hoefler\", \"Frere\", \"Jones\", \"commercial\", \"licensing\", \"Structure\", \"The\", \"ty\n"
     ]
    }
   ],
   "source": [
    "print(j[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
