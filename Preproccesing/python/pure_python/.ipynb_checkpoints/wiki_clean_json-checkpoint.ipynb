{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import bz2\n",
    "import xml.sax\n",
    "import mwparserfromhell\n",
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import csv\n",
    "from time import time\n",
    "from itertools import chain\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing.dummy import Pool as Threadpool\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Briggstone\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = set(['.', ',', ';', ':', '?', '!', '#', '\\\\', '/', '\"', '\\'', '\\'\\'', '´´', '´', '``', '`', '(', ')'])\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filters = punctuations.union(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content handler for the XML parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiXMLHandler(xml.sax.handler.ContentHandler):\n",
    "    \"\"\"Content handler for Wiki XML data using SAX\"\"\"\n",
    "    def __init__(self):\n",
    "        xml.sax.handler.ContentHandler.__init__(self)\n",
    "        self._buffer = None\n",
    "        self._values = {}\n",
    "        self._current_tag = None\n",
    "        self._previous_tag = None\n",
    "        self._pages = []\n",
    "        self._skip_page = False\n",
    "        self._punctuations = set(['.', ',', ';', ':', '?', '!', '#', '\\\\', '/', '\"', '\\'', '\\'\\'', '´´', '´', '``', '`', '(', ')'])\n",
    "        self._stop_words = set(stopwords.words('english'))\n",
    "        self._filter = self._punctuations.union(self._stop_words)\n",
    "        \n",
    "        \n",
    "    def characters(self, content):\n",
    "        \"\"\"Characters between opening and closing tags\"\"\"\n",
    "        if self._current_tag:\n",
    "            self._buffer.append(content)\n",
    "            \n",
    "            \n",
    "    def startElement(self, name, attrs):\n",
    "        \"\"\"Opening tag of element\"\"\"\n",
    "        if name in ('id', 'title', 'text'):\n",
    "            self._previous_tag = self._current_tag\n",
    "            self._current_tag = name\n",
    "            self._buffer = []\n",
    "            \n",
    "        \n",
    "    def endElement(self, name):\n",
    "        \"\"\"Closing tag of element\"\"\"\n",
    "        if name == self._current_tag:\n",
    "            if name == 'text':\n",
    "                if self._redirect():\n",
    "                    self._skip_page = True\n",
    "                    pass\n",
    "                else:\n",
    "                    self._skip_page = False\n",
    "                self._process_page()\n",
    "            elif name == 'id' and self._previous_tag == 'id':\n",
    "                pass\n",
    "            else:\n",
    "                self._values[name] = ' '.join(self._buffer)\n",
    "        if name == 'page':\n",
    "            if not self._skip_page:\n",
    "                self._pages.append((self._values['id'],\n",
    "                                    self._values['title'],\n",
    "                                    self._values['text'],\n",
    "                                    self._values['wikilinks']))\n",
    "                self._page_count = len(self._pages)\n",
    "    \n",
    "    \n",
    "    def _redirect(self):\n",
    "        wiki = mwparserfromhell.parse(self._buffer)\n",
    "        text = wiki.strip_code().split()\n",
    "        if len(text) == 0:\n",
    "            return False\n",
    "        return text[0] == 'REDIRECT'\n",
    "    \n",
    "    \n",
    "    def _process_page(self):\n",
    "        content = mwparserfromhell.parse(self._buffer)\n",
    "        self._values['wikilinks'] = [x.title.strip_code() for x in content.filter_wikilinks()]\n",
    "        content = mwparserfromhell.parse(content.strip_code().strip())\n",
    "        self._values['text'] = list(filter(\n",
    "            lambda word: word not in self._filter,\n",
    "            word_tokenize(content.strip_code().strip())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " 'E:/wikidata/Sample_zip/enwiki-20190220-pages-articles-multistream26.xml-p42567204p42663461.bz2')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folder = 'E:/wikidata/Sample_zip/'\n",
    "partitions = [data_folder + file for file in os.listdir(data_folder) if 'xml-p']\n",
    "len(partitions), partitions[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pages(data_path, save=True):\n",
    "    \"\"\"Finds and cleans all pages from a compressed wikipedia XML file\"\"\"\n",
    "    start = time()\n",
    "    # Object for handling xml\n",
    "    handler = WikiXMLHandler()\n",
    "\n",
    "    # Parsing object\n",
    "    parser = xml.sax.make_parser()\n",
    "    parser.setContentHandler(handler)\n",
    "\n",
    "    # Iteratively process file\n",
    "    i = 0\n",
    "    file = bz2.BZ2File(data_path, 'r')\n",
    "    for line in file:\n",
    "        try:\n",
    "            parser.feed(line)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        i += 1\n",
    "        if i > 1e+4: break\n",
    "    file.close()\n",
    "    if save:\n",
    "        temp = []\n",
    "        for i, page in enumerate(handler._pages):\n",
    "            temp.append([])\n",
    "            temp[i].append(page[0])\n",
    "            temp[i].append([page[1],page[2],page[3]])\n",
    "            \n",
    "        json = pd.DataFrame(temp)\n",
    "        json.to_json('test.json', orient = \"records\", lines = True)\n",
    "    \n",
    "    end = time()\n",
    "    print(f'\\n{data_path} preprocessed in {round(end-start)} seconds')\n",
    "    print(f'{handler._page_count} pages found in {data_path}')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Failed attempt to write a file in the same format as mrjob jsonprotocol\n",
    "## looks the same, but is not interpreted as such\n",
    "\n",
    "            with open('text', 'a', encoding = \"utf-8\") as fp:\n",
    "                for t in temp:\n",
    "                    s = '\\\"' + t[0] + '\\\"' + '\\t[\\\"' + t[1][0] + '\\\", ['\n",
    "                    for term in t[1][1]:\n",
    "                        s += '\\\"' + term + '\\\", '\n",
    "                    s = s[:-2]\n",
    "                    s+= '], ['\n",
    "                    for link in t[1][2]:\n",
    "                        s += '\\\"' + link + '\\\", '\n",
    "                    s = s[:-2]\n",
    "                    s += ']]'\n",
    "                    fp.write(s + '\\n')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "start = time()\n",
    "# Create a pool of workers to execute processes\n",
    "pool = Pool(processes = 4)\n",
    "\n",
    "# Map (service, task), applies function to each partition \n",
    "results = pool.map(preprocess_pages, partitions)\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "end = time()\n",
    "print(f'\\nWhole dump preprocessed in {round(end-start)} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "E:wikidata/enwiki-20190220-pages-articles-multistream14.xml-p7697599p7744799.bz2 preprocessed in 2 seconds\n",
      "86 pages found in E:wikidata/enwiki-20190220-pages-articles-multistream14.xml-p7697599p7744799.bz2\n"
     ]
    }
   ],
   "source": [
    "test_file = \"E:wikidata/enwiki-20190220-pages-articles-multistream14.xml-p7697599p7744799.bz2\"\n",
    "preprocess_pages(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
