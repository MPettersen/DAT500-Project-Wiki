{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pyspark\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from operator import add\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data inputs - Change input path for the files you want to process\n",
    "## To run, simply run all cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '/mnt/wiktorskit-danielb-ns0000k/home/notebook/group04/clean_data/wiki_test.csv'\n",
    "multi_path = '/mnt/wiktorskit-danielb-ns0000k/home/notebook/group04/clean_data/enwiki*.csv'\n",
    "nowiki_path = '/mnt/wiktorskit-danielb-ns0000k/home/notebook/group04/clean_data/nowiki*.csv'\n",
    "\n",
    "output_path_tf_idf = '/mnt/wiktorskit-danielb-ns0000k/home/notebook/group04/TF-IDF/tf_idf_21'\n",
    "nowiki_output_path_tf_idf = '/mnt/wiktorskit-danielb-ns0000k/home/notebook/group04/TF-IDF/tf_idf_nowiki_final_111'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPages(path):\n",
    "    \"\"\"\n",
    "    Reads a text file into a spark RDD if path is valid\n",
    "    \n",
    "    :param path: path of text file\n",
    "    :return: RDD containing the text file\n",
    "    \"\"\"\n",
    "    # Validity check of the path\n",
    "    for file in glob.glob(path):\n",
    "        if not os.path.isfile(file) or '/mnt/' not in file:\n",
    "            print('Not a valid path')\n",
    "            sys.exit(-1)\n",
    "    return sc.textFile('file://'+path)\n",
    "\n",
    "def getTerms(page):\n",
    "    \"\"\"\n",
    "    This function retrieves the terms from any given page\n",
    "    \n",
    "    :param page: a page represented as a tab seperated string\n",
    "    :return: list of terms belonging to page\n",
    "    \"\"\"\n",
    "    len_links = int(page.strip().split('\\t')[3])\n",
    "    term_list = page.strip().split('\\t')[4:-len_links:]\n",
    "    term_list = [x.lower() for x in term_list]\n",
    "    return term_list\n",
    "\n",
    "def get_IdTerm_tuple(key, terms):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    num_terms = len(terms)\n",
    "    if num_terms> 0:\n",
    "        for term in terms:\n",
    "            yield (key, term), 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procRDD(rdd, cache=False, part=True, hashp=True, npart=12):\n",
    "    \"\"\"\n",
    "    Helper to handle caching/partioning\n",
    "    \n",
    "    Function taken from:\n",
    "    https://stackoverflow.com/questions/31659404/spark-iteration-time-increasing-exponentially-when-using-join\n",
    "    \n",
    "    :param rdd: pyspark RDD\n",
    "    :param cache: boolean (default=True)\n",
    "    :param part: boolean (default=True)\n",
    "    :param hashp: boolean (default=True)\n",
    "    :param npart: number of partitions (default=12 suggested to be 2*(number of cores))\n",
    "    :return: rdd or rdd.cache()\n",
    "    \"\"\"\n",
    "    rdd = rdd if not part else rdd.repartition(npart)\n",
    "    rdd = rdd if not hashp else rdd.partitionBy(npart)\n",
    "    return rdd if not cache else rdd.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark.SparkContext.setSystemProperty('spark.executor.memory', '7g')\n",
    "# pyspark.SparkContext.setSystemProperty('spark.cores.max', '2')\n",
    "pyspark.SparkContext.setSystemProperty('spark.driver.cores', '2')\n",
    "pyspark.SparkContext.setSystemProperty('spark.driver.memory', '7g')\n",
    "sc = pyspark.SparkContext(appName='tf_idf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We want to find 4 parameters:\n",
    "n = term frequency in article\n",
    "N = total terms in article\n",
    "d = term frequency in corpus\n",
    "D = total size of corpus (number of articles)\n",
    "\n",
    "We calculate:\n",
    "TF * IDF = (n/N) * log (D/d)\n",
    "\n",
    "Alternative:\n",
    "\n",
    "TF * IDF = n * log(D/d)\n",
    "\n",
    "bias towards large documents\n",
    "\"\"\"\n",
    "npart = 16\n",
    "pages = getPages(nowiki_path)\n",
    "\n",
    "id_and_terms = pages \\\n",
    "    .filter(lambda line: len(line.strip()) > 0 and \\\n",
    "           not (line.strip().split('\\t')[1].find('File:') == 0) and \\\n",
    "           not (line.strip().split('\\t')[1].find('Wikipedia:') == 0)) \\\n",
    "    .map(lambda line: (line.strip().split('\\t')[0], \\\n",
    "                       getTerms(line)))\n",
    "\n",
    "# Create TF\n",
    "id_and_N = id_and_terms \\\n",
    "    .map(lambda key_value: (key_value[0], len(key_value[1])))\n",
    "TF = id_and_terms \\\n",
    "    .flatMap(lambda key_value : get_IdTerm_tuple(key_value[0], key_value[1])) \\\n",
    "    .reduceByKey(add) \\\n",
    "    .map(lambda key_value: (key_value[0][0], (key_value[0][1],key_value[1]))) \\\n",
    "    .join(id_and_N, npart) \\\n",
    "    .map(lambda key_value: \n",
    "         (key_value[1][0][0], \n",
    "          (key_value[0], key_value[1][0][1] / key_value[1][1])))\n",
    "\n",
    "# Create IDF\n",
    "D = id_and_terms.count()\n",
    "IDF = TF \\\n",
    "    .map(lambda key_value: (key_value[0], 1)) \\\n",
    "    .reduceByKey(add) \\\n",
    "    .map(lambda key_value: (key_value[0], log(D / key_value[1])))\n",
    "\n",
    "# Create TF_IDF\n",
    "TF_IDF = TF \\\n",
    "    .join(IDF, npart) \\\n",
    "    .map(lambda key_value: \n",
    "         (key_value[0], \n",
    "          (key_value[1][0][0], key_value[1][0][1] * key_value[1][1])))\n",
    "TF_IDF.saveAsTextFile(nowiki_output_path_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark.SparkContext.setSystemProperty('spark.executor.memory', '7g')\n",
    "# pyspark.SparkContext.setSystemProperty('spark.cores.max', '2')\n",
    "pyspark.SparkContext.setSystemProperty('spark.driver.cores', '2')\n",
    "pyspark.SparkContext.setSystemProperty('spark.driver.memory', '7g')\n",
    "sc = pyspark.SparkContext(appName='tf_idf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We do not have enough resources to use caching here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.SparkContext.setSystemProperty('spark.cores.max', '2')\n",
    "\"\"\"\n",
    "We want to find 4 parameters:\n",
    "n = term frequency in article\n",
    "N = total terms in article\n",
    "d = term frequency in corpus\n",
    "D = total size of corpus (number of articles)\n",
    "\n",
    "We calculate:\n",
    "TF * IDF = (n/N) * log (D/d)\n",
    "\n",
    "Alternative:\n",
    "\n",
    "TF * IDF = n * log(D/d)\n",
    "\n",
    "bias towards large documents\n",
    "\"\"\"\n",
    "npart = 16\n",
    "pages = getPages(nowiki_path)\n",
    "\n",
    "id_and_terms = pages \\\n",
    "    .filter(lambda line: len(line.strip()) > 0 and \\\n",
    "           not (line.strip().split('\\t')[1].find('File:') == 0) and \\\n",
    "           not (line.strip().split('\\t')[1].find('Wikipedia:') == 0)) \\\n",
    "    .map(lambda line: (line.strip().split('\\t')[0], \\\n",
    "                       getTerms(line)))\n",
    "D = id_and_terms.count()\n",
    "# Create TF\n",
    "id_and_N = procRDD(id_and_terms \\\n",
    "    .map(lambda key_value: (key_value[0], len(key_value[1]))), \n",
    "                   npart=npart)\n",
    "TF = id_and_terms \\\n",
    "    .flatMap(lambda key_value : get_IdTerm_tuple(key_value[0], key_value[1])) \\\n",
    "    .reduceByKey(add) \\\n",
    "    .map(lambda key_value: (key_value[0][0], (key_value[0][1],key_value[1]))) \\\n",
    "    .join(id_and_N, npart) \\\n",
    "    .map(lambda key_value: \n",
    "         (key_value[1][0][0], \n",
    "          (key_value[0], key_value[1][0][1] / key_value[1][1])))\n",
    "\n",
    "# Create IDF\n",
    "\n",
    "IDF = procRDD(TF \\\n",
    "    .map(lambda key_value: (key_value[0], 1)) \\\n",
    "    .reduceByKey(add) \\\n",
    "    .map(lambda key_value: (key_value[0], log(D / key_value[1]))),\n",
    "             npart=npart)\n",
    "\n",
    "# Create TF_IDF\n",
    "TF_IDF = TF \\\n",
    "    .join(IDF, npart) \\\n",
    "    .map(lambda key_value: \n",
    "         (key_value[0], \n",
    "          (key_value[1][0][0], key_value[1][0][1] * key_value[1][1])))\n",
    "TF_IDF.saveAsTextFile(nowiki_output_path_tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF is the final output. It looks like this: (term, (article_id, tf-idf.score)), the key term is repeated for different article-ids\n",
    "### output to file might be term -> [(id_i,score_i),....,(id_n,score_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTFIDF(line):\n",
    "    tf_idf = line.split(',')\n",
    "    if len(tf_idf) == 3:\n",
    "        return tf_idf[0][2:-1], (tf_idf[1].strip()[2:-1], float(tf_idf[2][2:-1].strip()[:-2]))\n",
    "    else:\n",
    "        return ','.join(tf_idf[:-2])[2:-1], (tf_idf[-2].strip()[2:-1], float(tf_idf[-1][2:-1].strip()[:-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark.SparkContext.setSystemProperty('spark.executor.memory', '7g')\n",
    "pyspark.SparkContext.setSystemProperty('spark.driver.cores', '2')\n",
    "pyspark.SparkContext.setSystemProperty('spark.driver.memory', '7g')\n",
    "pyspark.SparkContext.setSystemProperty('spark.cleaner.ttl', '600')\n",
    "sc = pyspark.SparkContext(appName='pagerank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = sc.textFile('file:///mnt/wiktorskit-danielb-ns0000k/home/notebook/group04/TF-IDF/tf_idf_nowiki_final/part*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75436783,\n",
       " [\"('oversvømmet', ('847877', 0.0028516679896155024))\",\n",
       "  \"('oversvømmet', ('1520687', 0.0661727219557499))\",\n",
       "  \"('oversvømmet', ('1577312', 0.002659114650395747))\",\n",
       "  \"('oversvømmet', ('30336', 0.0027072676319924507))\",\n",
       "  \"('oversvømmet', ('351260', 0.008515898817090176))\"])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf.count(), tf_idf.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"('oversvømmet', ('847877', 0.0028516679896155024))\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf.take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75436783,\n",
       " [('oversvømmet', ('847877', 0.002851667989615502)),\n",
       "  ('oversvømmet', ('1520687', 0.066172721955749)),\n",
       "  ('oversvømmet', ('1577312', 0.00265911465039574)),\n",
       "  ('oversvømmet', ('30336', 0.00270726763199245)),\n",
       "  ('oversvømmet', ('351260', 0.00851589881709017))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TF_IDF = tf_idf.map(lambda line: getTFIDF(line))\n",
    "TF_IDF.count(), TF_IDF.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', <pyspark.resultiterable.ResultIterable object at 0x7fbbb20a89e8>), ('a', <pyspark.resultiterable.ResultIterable object at 0x7fbbb20a8ac8>)]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([(\"a\", ('2', 1)), (\"b\", 1), (\"a\", (3, 1))])\n",
    "a = x.groupByKey().collect()\n",
    "print(a)\n",
    "# for i in a:\n",
    "#     for j in i:\n",
    "#         print(j)\n",
    "#         if not isinstance(j, str):\n",
    "#             for k in j:\n",
    "#                 print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pyspark.rdd.PipelinedRDD, pyspark.rdd.RDD)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(TF_IDF), type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF.saveAsTextFile(nowiki_output_path_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"13.48,88'\", [('531769', 0.0253389780250443)]),\n",
       " (\"149,115'\", [('23711', 0.01527833714820648)]),\n",
       " (\"OppLillehammerLitrim20083603,6Sjur'\", [('657417', 0.0189684440017987)]),\n",
       " (\"32.31,55'\", [('533924', 0.0402085579439326)]),\n",
       " (\"xooooxxx4,10'\", [('1626423', 0.0344350214186)])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = TF_IDF.groupByKey().mapValues(list)\n",
    "grouped.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/IPython/core/async_helpers.py\u001b[0m in \u001b[0;36m_pseudo_sync_runner\u001b[0;34m(coro)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \"\"\"\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_async\u001b[0;34m(self, raw_cell, store_history, silent, shell_futures)\u001b[0m\n\u001b[1;32m   2946\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstore_history\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2947\u001b[0m             self.history_manager.store_inputs(self.execution_count,\n\u001b[0;32m-> 2948\u001b[0;31m                                               cell, raw_cell)\n\u001b[0m\u001b[1;32m   2949\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2950\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_cell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/IPython/core/history.py\u001b[0m in \u001b[0;36mstore_inputs\u001b[0;34m(self, line_num, source, source_raw)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_hist_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb_input_cache_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb_input_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m             \u001b[0;31m# Trigger to flush cache and write to DB.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "grouped.take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
